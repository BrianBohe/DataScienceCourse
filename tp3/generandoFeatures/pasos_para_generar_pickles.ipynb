{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "import statistics as sts\n",
    "import gc as gc\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "## Aprovechamos los .hdf generados a partir de los .mat en el tp anterior.\n",
    "path = \"../../tp2/datos/\"\n",
    "load_path = path + \"/{}.hdf\"\n",
    "\n",
    "##Electrodos de interes\n",
    "electrodos = [8, 44, 80, 131, 185]\n",
    "\n",
    "# Cantidad de pacientes S y P\n",
    "N_P = 2\n",
    "N_S = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Script Generador de Features</h1>\n",
    "<blockquote>\n",
    "    <p>Este script genera los features que luego utilizaremos en el análisis univariado y multivariado.</p>\n",
    "    <p>Estas son:\n",
    "    <ul>\n",
    "        <li>Potencia para cada banda de frecuencia (Delta, Theta, Alpha, Beta y Gamma)</li>\n",
    "        <li>Potencia normalizada para las mismas bandas de frecuencia.</li>\n",
    "        <li>Una medida de información intra-electrodo</li>\n",
    "        <li>Una medida de información inter-electrodo</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <p>Cada marcador se computa por cada epochs y luego se toma la media y el desvío estándard entre los valores de cada epochs. En el caso de los features espectrales, para el cómputo de un epoch primero se promedian las frecuencias entre los mismos electrodos utilizados en el TP2.</p>\n",
    "    <p>Como medida intra-electrodo se decide utilizar la fuente de información modelada en el TP2 considerando sólo al electrodo 8. Se decide esto porque en los resultados observados presentó una mejor diferencia en los rangos de entropía para entre los pacientes P y S.</p>\n",
    "    <p>La medida inter-electrodo es la entropía de la fuente modelada con un alfabeto general para todos los electrodos (lo mismo que se utilizó en el TP2).</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Construyendo alfabetos</h4>\n",
    "<blockquote>\n",
    "<p>Primero vamos a generar al igual que en el TP anterior los alfabetos que se utilzarán luego en las medidas inter/intra-electrodo.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pacientes_P = []\n",
    "pacientes_S = []\n",
    "for load_name, N, dest, offset in [(\"P\", N_P, pacientes_P, 0), (\"S\", N_S, pacientes_S, 10)]:\n",
    "    for i in range(1, 1 + N):\n",
    "        paciente = load_name + \"{:02d}\".format(i)\n",
    "        df_ = pd.read_hdf(load_path.format(paciente))\n",
    "        df_ = df_.loc[offset + i-1,:,electrodos,:]\n",
    "        dest.append(df_)\n",
    "\n",
    "pd_pacientes = pd.concat(pacientes_P + pacientes_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pacientes_b_mean = []\n",
    "N_b = 0\n",
    "step_b = 0\n",
    "alfabeto_8 = []\n",
    "\n",
    "# Calculo de N (cantidad de bins): Se usa n = 201 porque los epochs tienen esa cantidad de muestras\n",
    "def calculo_N(df_):\n",
    "    return math.ceil((df_.max() - df_.min()) / (3.5*df_.std() * 201 ** (-1/3)))\n",
    "\n",
    "def calcular_simbolos_sensor_8_para_promediando_entre_pacientes(pacientes):\n",
    "    pacientes_8 = pacientes.loc[pd.IndexSlice[:,:,8,:],:]\n",
    "    val = pacientes_8.groupby([\"epoch\"])['valores'].mean()\n",
    "    i_s = pd.DataFrame({'sensor':[8]})\n",
    "    i_s['N'] = calculo_N(val)\n",
    "    i_s['min'] = val.min()\n",
    "    i_s['max'] = val.max()\n",
    "    i_s['step'] = (i_s['max'] - i_s['min']) / i_s['N']\n",
    "    return i_s\n",
    "\n",
    "# Calcular N para hacer la transformación simbólica\n",
    "def calcular_simbolos_inter_promediando_entre_pacientes(pacientes):\n",
    "    pacientes_b_mean = pd.concat(pacientes_P + pacientes_S).groupby([\"sensor\",\"tiempo\"]).mean()\n",
    "    N_b = math.ceil(pacientes_b_mean.max() - pacientes_b_mean.min() / (pacientes_b_mean.std() * len(pacientes_b_mean) ** (-1/3)))\n",
    "    step_b = (pacientes_b_mean.max() - pacientes_b_mean.min()) / N_b\n",
    "    return pacientes_b_mean, N_b, step_b\n",
    "\n",
    "def calcular_entropia_inter_electrodo_para_un_epoch(df_):\n",
    "    df_['simbolos_inter'] = (df_[\"valores\"] - pacientes_b_mean['valores'].min()) // step_b['valores']\n",
    "    df_[df_['simbolos_inter'] < 0] = 0\n",
    "    df_[df_['simbolos'] > N_b] = N_b\n",
    "    df_['repeticiones_inter'] = df_.groupby([\"simbolos_inter\"]).transform('count')['valores']\n",
    "    df_['probabilidad_inter'] = df_['repeticiones_inter'] / len(df_)\n",
    "    p = df_[\"probabilidad_inter\"]\n",
    "    return -sum(p * np.log(p))\n",
    "    \n",
    "def calcular_entropia_para_un_epoch(df_):\n",
    "    df_['simbolos_intra'] = (df_['valores'].values - alfabeto_8['min'].values) // alfabeto_8['step'].values\n",
    "    df_[df_['simbolos_intra'] < 0] = 0\n",
    "    df_[df_['simbolos_intra'] > alfabeto_8['N'].values[0]] = alfabeto_8['N'].values[0]\n",
    "    df_['repeticiones_intra'] = df_.groupby([\"simbolos_intra\"]).transform('count')['valores']\n",
    "    df_['probabilidad_intra'] = df_['repeticiones_intra'] / len(df_)\n",
    "    df_ = df_.groupby(\"simbolos_intra\").first()\n",
    "    p = df_[\"probabilidad_intra\"]\n",
    "    return -sum(p * np.log(p))\n",
    "\n",
    "# Se calcula la cantidad de Bins promediando entre todos los pacientes\n",
    "alfabeto_8 = calcular_simbolos_sensor_8_para_promediando_entre_pacientes(pd_pacientes)\n",
    "\n",
    "# Para el caso de la medida inter-electrodo se calcula la cantidad de Bins entre todos los electrodos y pacientes \n",
    "pacientes_b_mean, N_b, step_b = calcular_simbolos_inter_promediando_entre_pacientes(pd_pacientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Establecimos este orden para los features\n",
    "def give_expected_ordered_keys():\n",
    "    ordered_keys = []\n",
    "    for i, forma_de_calcular_feature in enumerate(['media_','std_']):\n",
    "        for  j, tipo_de_banda in enumerate(['delta', 'theta', 'alpha', 'beta', 'gamma', 'delta_norm', 'theta_norm', 'alpha_norm', 'beta_norm', 'gamma_norm', 'intra', 'inter']):\n",
    "            ordered_keys.append(\"{}{}\".format(forma_de_calcular_feature,tipo_de_banda))\n",
    "    return np.array(ordered_keys)\n",
    "\n",
    "##  Y así será al forma de los archivos pickle generados con los features (en crudo y normalizadas)\n",
    "def numpy_to_pickle(aCollectionOfNumpyArray, aPickelFileName):\n",
    "    print(\"Creando index...\")\n",
    "    lenght = int(len(aCollectionOfNumpyArray)/2)\n",
    "    arrays_index = [\n",
    "        ['P']*N_P+['S']*N_S,\n",
    "        [i for j in range(2) for i in range(lenght)]\n",
    "    ]\n",
    "\n",
    "    index = pd.MultiIndex.from_arrays(arrays_index, names=[\"tipo\", \"indice_paciente\"])\n",
    "    arrays_columns = [\n",
    "        ['media']*12+['std']*12,\n",
    "        ['delta', 'theta', 'alpha', 'beta', 'gamma', 'delta_norm', 'theta_norm', 'alpha_norm', 'beta_norm', 'gamma_norm', 'intra', 'inter']*2\n",
    "    ]\n",
    "\n",
    "    index_columns = pd.MultiIndex.from_arrays(arrays_columns, names=[\"agrupacion_feature\",\"feature\"])\n",
    "\n",
    "    df = pd.DataFrame(data=aCollectionOfNumpyArray, index=index, columns=index_columns)\n",
    "\n",
    "    print(df)\n",
    "\n",
    "    df.to_pickle(\"../{}.pickle\".format(aPickelFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Definimos el orden de los atributos antes de codear de forma rapida\n",
    "##Esta clase esta solo para asegurar ese orden.\n",
    "class Sample():\n",
    "\n",
    "    def __init__(self, featuresDict):\n",
    "        self.features = {}\n",
    "\n",
    "        for k,v in featuresDict.items():\n",
    "            self.features[k] = v\n",
    "                \n",
    "    def show_features_as_np(self):\n",
    "        valores = []\n",
    "        for key in give_expected_ordered_keys():\n",
    "            valores.append(self.features[\"{}\".format(key)])\n",
    "        \n",
    "        return np.array(valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo la potencia de cada banda\n",
    "def calcular_potencia_por_bandas_de_frecuencias(f, P):\n",
    "    return {\n",
    "        'delta': P[f<4].sum(),\n",
    "        'theta': P[(4<=f) & (f<8)].sum(),\n",
    "        'alpha': P[(8<=f) & (f<13)].sum(),\n",
    "        'beta' : P[(13<=f) & (f<30)].sum(),\n",
    "        'gamma': P[30<=f].sum()\n",
    "    }\n",
    "\n",
    "#  Dado un diccionario con pontencia de cada banda, \n",
    "#      calculo la potencia normalizada de cada banda.\n",
    "def normalizar_banda(bandas):\n",
    "    suma_del_poder_total = sum(bandas.values()) \n",
    "    bandas_norm = {}\n",
    "\n",
    "    for key, value in bandas.items():\n",
    "        bandas_norm[\"{}_norm\".format(key)]= bandas[key]/suma_del_poder_total\n",
    "\n",
    "    return bandas_norm\n",
    "\n",
    "def computar_promedio_y_desvio_por_marcador_sobre_todos_los_epochs(lista_de_marcadores_por_epoch):\n",
    "    features_paciente = {}\n",
    "\n",
    "    #Junto todos los valores de un mismo marcador para usar mean y stdev de statistics\n",
    "    for key, value in lista_de_marcadores_por_epoch[0].items():\n",
    "        \n",
    "        _values = np.array([])\n",
    "        \n",
    "        for dic in lista_de_marcadores_por_epoch:\n",
    "            _values = np.append(_values, dic[key])\n",
    "\n",
    "        _mean = sts.mean(_values)\n",
    "        _stdev = sts.stdev(_values,xbar=_mean)\n",
    "\n",
    "        features_paciente[\"media_{}\".format(key)] = _mean\n",
    "        features_paciente[\"std_{}\".format(key)] = _stdev\n",
    "\n",
    "    return Sample(features_paciente)\n",
    "\n",
    "# De cada muestra o sample tomamos 24 features ya elegidos, computados a través del promedio y varianza\n",
    "#      entre los epochs de cada paciente.\n",
    "def calcular_features_de_un_paciente(paciente):\n",
    "    \n",
    "    marcadores_de_este_epoch = {}\n",
    "\n",
    "    ## Junto la informacion por cada epoch\n",
    "    p_ = paciente.groupby(['epoch','tiempo']).mean()\n",
    "    \n",
    "    lista_de_epochs = list( set(p_.loc[:,:].index.get_level_values('epoch',)) )\n",
    "    lista_de_marcadores_por_epoch = []\n",
    "    \n",
    "    for epoch in lista_de_epochs:\n",
    "        \n",
    "        frecuencias = p_.loc[epoch,:]\n",
    "        f, P = scipy.signal.welch(frecuencias['valores'], fs=250, nperseg=201)\n",
    "        \n",
    "        # Calculo todos los marcadores que necesito para un epoch\n",
    "        # Estos son las bandas, las bandas normalizadas y alguna medida intra/inter electrodo\n",
    "        marcadores_de_este_epoch = calcular_potencia_por_bandas_de_frecuencias(f,P)\n",
    "        marcadores_de_este_epoch.update(normalizar_banda(marcadores_de_este_epoch))\n",
    "        \n",
    "        # Para calcular el marcador intra-electrodo solo necesito la frecuencia del sensor/electrodo 8\n",
    "        frecuencias_8 = paciente.loc[pd.IndexSlice[:,epoch,8,:],:].groupby(['epoch','tiempo']).mean()\n",
    "        marcadores_de_este_epoch.update({\"intra\":calcular_entropia_para_un_epoch(frecuencias_8)})\n",
    "        \n",
    "        #TODO: Cambio de Intra Inter\n",
    "        marcadores_de_este_epoch.update({\"inter\":calcular_entropia_inter_electrodo_para_un_epoch(frecuencias)})\n",
    "        lista_de_marcadores_por_epoch.append( marcadores_de_este_epoch )\n",
    "    \n",
    "    ## Ahora calculo los features tomando promedio y desvio sobre los marcadores de cada epoch\n",
    "    sample = computar_promedio_y_desvio_por_marcador_sobre_todos_los_epochs(lista_de_marcadores_por_epoch)\n",
    "\n",
    "    return sample\n",
    "\n",
    "#   Usamos los mismos archivos *.hdf generados para el TP anterior.\n",
    "def levantar_hdf(load_name, nth):\n",
    "    paciente = load_name + \"{:02d}\".format(i)\n",
    "    return pd.read_hdf(load_path.format(paciente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pacientes = []\n",
    "\n",
    "pacientes = list(pacientes_P + pacientes_S)\n",
    "for paciente in pacientes:\n",
    "    samples_pacientes.append(calcular_features_de_un_paciente(paciente))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_pacientes[0].show_features_as_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(pacientes_P + pacientes_S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miremos como están quedando los datos. Un buen grafico para ver que tan esparsos son los valores de cada feature estaría bueno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Codigo para imprimir como barplot, violinplot y swarmplot un DataFrame\n",
    "def analisis_comparativo(df_banda, df_banda_estandarizado):\n",
    "#    ymin = min(df_banda['Valores'])\n",
    "#    ymax = max(df_banda['Valores'])\n",
    "#    decimo = (ymax - ymin)/len(df_banda['Valores'])\n",
    "#    ymin, ymax = ymin - decimo, ymax + decimo\n",
    "\n",
    "    # Hay que tener en cuenta que son pocos valores\n",
    "\n",
    "    #ax = sns.violinplot(x=\"Features\", y=\"Valores\", hue=\"Capacidad cognitiva\", data=df_b,  split=True, palette=\"Set2\", inner=\"stick\", cut=0)\n",
    "    #sns.plt.show()\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, sharey=True, figsize=(12,4))\n",
    "    \n",
    "    #ax1\n",
    "#    sns.pointplot(\"who\", \"age\", data=titanic, join=False,n_boot=10, ax=ax1)\n",
    "\n",
    "    sns.swarmplot(x=\"Sin Estandarizar\", y=\"Valores\", hue=\"Capacidad cognitiva\",data=df_banda,  split=True, palette=\"Set2\", size=4, ax=ax1)\n",
    "#    ax1.set_ylim([ymin, ymax])\n",
    "    \n",
    "    sns.swarmplot(x=\"Estandarizado\", y=\"Valores\", hue=\"Capacidad cognitiva\", data=df_banda_estandarizado,  split=True, palette=\"Set2\", size=4, ax=ax2)\n",
    "    sns.plt.show()\n",
    "\n",
    "    #sns.barplot(x=\"Features\", y=\"Valores\", hue=\"Capacidad cognitiva\", data=df_b, palette=\"Set2\")\n",
    "    #ax = sns.swarmplot(x=\"Features\", y=\"Valores\", hue=\"Capacidad cognitiva\", split=True, data=df_b, palette=\"Set2\")\n",
    "    #ax.set_ylim([ymin, ymax])\n",
    "    \n",
    "    #handles, labels = ax.get_legend_handles_labels()\n",
    "    #l = ax.legend(handles[:2], labels[:2])\n",
    "    #l.set_title(\"Capacidad cognitiva\", prop = {'size':'small'})\n",
    "    #sns.plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imprimo por feature para ver como quedaron los datos\n",
    "def printear_comparacion_sin_y_con_estandarizar(nd_datos, nd_datos_stand, lista_de_keys):\n",
    "    \n",
    "    for indice, key in enumerate(lista_de_keys):\n",
    "        valores = list([nd[indice] for nd in nd_datos])\n",
    "        \n",
    "        valores_estandarizados = list([nd[indice] for nd in nd_datos_stand])\n",
    "        \n",
    "        keys = [key]* (N_P+N_S)\n",
    "\n",
    "        df_banda = pd.DataFrame({\n",
    "            \"Capacidad cognitiva\": ([\"Disminuída\"] * N_P) + ([\"Normal\"] * N_S),\n",
    "            \"Sin Estandarizar\": keys,\n",
    "            \"Valores\": valores\n",
    "        })\n",
    "        \n",
    "        df_banda_estandarizada = pd.DataFrame({\n",
    "            \"Capacidad cognitiva\": ([\"Disminuída\"] * N_P) + ([\"Normal\"] * N_S),\n",
    "            \"Estandarizado\": keys,\n",
    "            \"Valores\": valores_estandarizados\n",
    "        })\n",
    "        #print (\"resultado \\n\")\n",
    "        #print (df_for_one_feature)\n",
    "        analisis_comparativo(df_banda, df_banda_estandarizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_features_por_paciente = []\n",
    "for muestra in samples_pacientes:\n",
    "    np_features_por_paciente.append( muestra.show_features_as_np() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todas las keys encadenadas\n",
    "#print (len ( list(itertools.chain(*[list(give_expected_ordered_keys()) for _ in range(N_P+N_S)]))) )\n",
    "# todos los valores encadenados\n",
    "#print (len(list(itertools.chain(*[list(paciente) for paciente in np_features_por_paciente]))) )\n",
    "# todos los headers encadenados\n",
    "#print (len( ([\"Reducida\"] * (N_P * 24)) + ([\"Normal\"] * (N_S * 24))) )\n",
    "\n",
    "#df_features_compress = pd.DataFrame({\n",
    "#    \"Capacidad cognitiva\": ([\"Reducida\"] * (N_P * 24)) + ([\"Normal\"] * (N_S * 24)),\n",
    "#    \"Features\": list(itertools.chain(*[list(give_expected_ordered_keys()) for _ in range(N_P+N_S)])),\n",
    "#    \"Valores\": list(itertools.chain(*[list(paciente) for paciente in np_features_por_paciente]))\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_features_por_paciente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos un pickle con los features en bruto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_pickle(np.array(np_features_por_paciente, copy=True), \"df_features_prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos también ahora un pickle con las/los features pero ahora estandarizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_np_copy = np.array(np_features_por_paciente, copy=True)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(a_np_copy)\n",
    "#print (scaler)\n",
    "np_features_por_paciente_norm = scaler.transform(a_np_copy)\n",
    "\n",
    "print (\"media: \")\n",
    "print(scaler.mean_)\n",
    "print (\"std: \")\n",
    "print(scaler.var_)\n",
    "print (\"samples: {}\".format(scaler.n_samples_seen_))\n",
    "\n",
    "#scalerDelNorm = preprocessing.StandardScaler().fit(np_features_por_paciente_norm)\n",
    "## Esto deberia devolver un vector de medias con 0s y uno de desvios con 1s, pero la media no los da.\n",
    "#print (\"media: \")\n",
    "#print(scalerDelNorm.mean_)\n",
    "#print (\"std: \")\n",
    "#print(scalerDelNorm.var_)\n",
    "\n",
    "#unPasoMasScalerDelNorm = preprocessing.StandardScaler().fit(scalerDelNorm.transform(np_features_por_paciente_norm))\n",
    "## Esto deberia devolver un vector de medias con 0s y uno de desvios con 1s\n",
    "#print (\"media: \")\n",
    "#print(unPasoMasscalerDelNorm.mean_)\n",
    "#print (\"std: \")\n",
    "#print(unPasoMasscalerDelNorm.var_)\n",
    "\n",
    "#df_features_normalizado_compress = pd.DataFrame({\n",
    "#    \"Capacidad cognitiva\": ([\"Reducida\"] * (N_P * 24)) + ([\"Normal\"] * (N_S * 24)),\n",
    "#    \"Features\": list(itertools.chain(*[list(give_expected_ordered_keys()) for _ in range(N_P+N_S)])),\n",
    "#    \"Valores\": list(itertools.chain(*[list(paciente) for paciente in np_features_por_paciente_norm]))\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (df_features_normalizado_compress)\n",
    "printear_comparacion_sin_y_con_estandarizar(np_features_por_paciente, np_features_por_paciente_norm, give_expected_ordered_keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_to_pickle(np_features_por_paciente_norm, \"df_features_estandarizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "<p>Aunque visualmente no lo parezca, el desvío quedó en 1 y la medía más cerca del 0 (aunque no en 0). Algo a tener en cuenta es que estamos estandarizando por <i>feature</i>, y eso incluye tanto a los pacientes P como a los S.  \n",
    "La apertura de los valores tiene sólo sentido viendo que los valores antes de estandarizar son muy pequeños.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([p[0] for p in np_features_por_paciente]))\n",
    "print(\"media {}\".format(np.mean(np.array([p[0] for p in np_features_por_paciente]))))\n",
    "print(\"std {}\".format(np.std(np.array([p[0] for p in np_features_por_paciente]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([p[0] for p in np_features_por_paciente_norm]))\n",
    "print(\"media {}\".format(np.mean(np.array([p[0] for p in np_features_por_paciente_norm]))))\n",
    "print(\"std {}\".format(np.std(np.array([p[0] for p in np_features_por_paciente_norm]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
