{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import nltk\n",
    "from functools import reduce\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "import numpy\n",
    "import math\n",
    "from matplotlib.ticker import ScalarFormatter \n",
    "import random\n",
    "#import word2vec\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, remove_stopwords=True, allowed_words = [], lemat=False):\n",
    "    stopWords = set(stopwords.words('english'))|set([\"''\",\",''\",\".''\",\".,'\",\"...\",\".,\",'``','--'])\n",
    "    tokens = nltk.wordpunct_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    tokens = [word for word in tokens if (not (word in stopWords and remove_stopwords) or word in allowed_words)\n",
    "              and not word.isdigit()]\n",
    "    \n",
    "    if lemat:\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        tokens = [porter_stemmer.stem(word) for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.ElementTree(file='ap/ap.xml').getroot()\n",
    "\n",
    "news_dic = {}\n",
    "full_text = []\n",
    "for node in root:\n",
    "    docno, text = node\n",
    "    news_dic[docno.text.strip()] = text.text\n",
    "\n",
    "full_text = ''.join([v for k,v in news_dic.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenize(full_text)\n",
    "fd = nltk.FreqDist(tokens)\n",
    "most_common_s = set(sorted(fd, key=lambda x: fd[x],reverse=True)[:500])\n",
    "most_common_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "bgm    = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(tokens, window_size)\n",
    "finder.apply_ngram_filter(lambda w1, w2: w1 not in most_common_s or w2 not in most_common_s)\n",
    "scored = finder.score_ngrams( bgm.pmi  )\n",
    "scored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_species_f = open('ap/pg1228.txt', 'r')\n",
    "inputfile = orig_species_f.read()\n",
    "tokens_orig = tokenize(inputfile, False, allowed_words = ['for'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.draw.dispersion.dispersion_plot(tokens_orig,['plant','instinct','for'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apps_for_word_in_text(aWord, aText):\n",
    "    return [i for i,word in enumerate(aText) if word == '{}'.format(aWord)]\n",
    "\n",
    "instinct_app = get_apps_for_word_in_text('instinct', tokens_orig)\n",
    "plant_app = get_apps_for_word_in_text('plant', tokens_orig)\n",
    "for_app = get_apps_for_word_in_text('for', tokens_orig)\n",
    "\n",
    "total_words = len(tokens_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que termine usamos un salto cada 10 palabras.\n",
    "Lo siguiente lo corrimos y guardamos en el dir \"ap/guassian_sum_calc/*.out\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## rowo is an acronym of \"rate of word occurrency\"\n",
    "rowo_instinct = []\n",
    "rowo_plant = []\n",
    "rowo_for = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Write files\n",
    "intervalo = 10\n",
    "\n",
    "def sum_gaussians_in(aPos, apps):\n",
    "    stdev = 50\n",
    "    return sum([norm.pdf(aPos,j,50) for j in apps])\n",
    "\n",
    "for _rowo,_app, _word in [\n",
    "    (rowo_instinct,instinct_app,'instict'),\n",
    "    (rowo_plant,plant_app,'plant'),\n",
    "    (rowo_for,for_app,'for')]:\n",
    "    \n",
    "    _rowo.extend([sum_gaussians_in(i,_app) for i in range(0,total_words,intervalo)])\n",
    "    \n",
    "    with open('ap/gaussian_sum_calc/{}.out'.format(_word),'wb') as fp:\n",
    "        pickle.dump(_rowo, fp)\n",
    "    print (\"{} ready.\".format(_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read files\n",
    "for _rowo,_word in [(rowo_instinct,'instict'),(rowo_plant,'plant'),(rowo_for,'for')]:\n",
    "    with open('ap/gaussian_sum_calc/{}.out'.format(_word),'rb') as fp:\n",
    "        _rowo.extend(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.plt.subplot(311)\n",
    "sns.plt.plot(rowo_instinct)\n",
    "sns.plt.subplot(312)\n",
    "sns.plt.plot(rowo_plant)\n",
    "sns.plt.subplot(313)\n",
    "sns.plt.plot(rowo_for)\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autocorrelacion(_row):\n",
    "    pwt_avg = sum([ _row[pos] for pos in range(len(_row)) ]) / len(_row)\n",
    "    cociente = pwt_avg**2 + sum([ _row[pos]**2 for pos in range(len(_row)) ]) / len(_row)\n",
    "\n",
    "    taus = []\n",
    "\n",
    "    for tau in range(1000):\n",
    "        pwt_tau_avg = sum([ _row[pos]*_row[pos+tau] for pos in range(0,len(_row)-tau)]) / (len(_row)-tau)\n",
    "        taus.append(pwt_tau_avg - pwt_avg * (sum([_row[pos+tau] for pos in range(0,len(_row)-tau)]) / (len(_row)-tau)))\n",
    "        taus[tau] /= cociente\n",
    "    return taus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_rowo_instinct = autocorrelacion(rowo_instinct)\n",
    "sns.plt.subplot(311)\n",
    "sns.plt.plot(auto_rowo_instinct)\n",
    "\n",
    "auto_rowo_plant = autocorrelacion(rowo_plant)\n",
    "sns.plt.subplot(312)\n",
    "sns.plt.plot(auto_rowo_plant)\n",
    "\n",
    "auto_rowo_for = autocorrelacion(rowo_for)\n",
    "sns.plt.subplot(313)\n",
    "sns.plt.plot(rowo_for)\n",
    "\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fd_orig = nltk.FreqDist(tokens_orig)\n",
    "salto = len(fd_orig)//1000\n",
    "elegidas = dict()\n",
    "fd_orig_s = sorted(fd_orig, key=lambda x: fd_orig[x],reverse=True)[:100]\n",
    "fd_orig_s\n",
    "for e in fd_orig_s:\n",
    "        elegidas[e] = fd_orig[e]\n",
    "\n",
    "elegidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def freq_in_partition(w,p,p_total,text):\n",
    "    desde = p*(len(text)//p_total)\n",
    "    hasta = (p+1)*(len(text)//p_total)\n",
    "    tot = 0\n",
    "    for wt in text[desde:hasta]:\n",
    "        if w==wt:\n",
    "            tot += 1\n",
    "    return tot\n",
    "\n",
    "particiones = 64\n",
    "entropias_orig = dict()\n",
    "for w,freq in elegidas.items():\n",
    "    entropia_w = 0\n",
    "    for p in range(particiones):\n",
    "        prob = freq_in_partition(w,p,particiones,tokens_orig)/freq\n",
    "        if prob != 0:\n",
    "            entropia_w += prob*(math.log(prob,2))\n",
    "    entropia_w = -1*entropia_w\n",
    "    entropias_orig[w] = entropia_w\n",
    "\n",
    "\n",
    "tokens_shuffled = tokens_orig.copy()\n",
    "random.shuffle(tokens_shuffled)\n",
    "entropias_random = dict()\n",
    "for w,freq in elegidas.items():\n",
    "    entropia_w = 0\n",
    "    for p in range(particiones):\n",
    "        prob = freq_in_partition(w,p,particiones,tokens_shuffled)/freq\n",
    "        if prob != 0:\n",
    "            entropia_w += prob*(math.log(prob,2))\n",
    "    entropia_w = -1*entropia_w\n",
    "    entropias_random[w] = entropia_w\n",
    "\n",
    "entropias_orig\n",
    "entropias_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = []\n",
    "entropies = []\n",
    "entropies_shuff = []\n",
    "for w,freq in elegidas.items():\n",
    "    frequencies.append(freq)\n",
    "    entropies.append(entropias_orig[w])\n",
    "    entropies_shuff.append(entropias_random[w])\n",
    "\n",
    "h = sns.pointplot(x=frequencies,y=entropies, join = False, ci = None)\n",
    "g = sns.pointplot(x=frequencies,y=entropies_shuff, join = False, ci = None, color=\"red\")\n",
    "sns.mpl.rc(\"figure\", figsize=(15,10))\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary(news_dic.values())\n",
    "lsi = models.LsiModel([dictionary.doc2bow(text) for text in news_dic.values()], id2word=dictionary, num_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lsi(word):\n",
    "    vec_bow = dictionary.doc2bow(word.lower())\n",
    "    return lsi[vec_bow] # convert the query to LSI space \n",
    "lsi.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.Word2Vec(news_dic.values(), size=15, window=20, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sim_f = open('wordsim353_sim_rel/wordsim_similarity_goldstandard.txt')\n",
    "error = 0\n",
    "for line in word_sim_f:\n",
    "    w1, w2, v2 = line.split()\n",
    "    if w1 in model.wv.vocab and w2 in model.wv.vocab:\n",
    "        v2 = float(v2)\n",
    "        v1 = model.wv.similarity(w1, w2)*10\n",
    "        epsilon = (v1-v2)**2\n",
    "        error += epsilon\n",
    "    print(w1,w2,v1,v2,epsilon)\n",
    "\n",
    "error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
